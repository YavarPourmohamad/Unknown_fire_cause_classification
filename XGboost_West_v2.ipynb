{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.metrics as metrics\n",
    "import scipy.cluster.hierarchy as Sciplot\n",
    "import shap\n",
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading FPA FOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPA_FOD_P = pd.read_csv(filepath_or_buffer = '/bsuhome/yavarpourmohamad/scratch/Summer_2022/Aggregated/FPA_FOD_Plus.csv',\n",
    "                        sep = ',', low_memory = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting State and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['WA', 'OR', 'CA', 'ID', 'NV', 'MT', 'WY', 'UT', 'AZ', 'CO', 'NM']\n",
    "FPA_FOD_W = FPA_FOD_P.loc[FPA_FOD_P['STATE'].isin(states)]\n",
    "FPA_FOD_W = FPA_FOD_W.loc[FPA_FOD_W['LONGITUDE'] < -102.05]\n",
    "\n",
    "cols = ['DISCOVERY_DOY', 'FIRE_YEAR', 'STATE', 'FIPS_CODE', 'NWCG_GENERAL_CAUSE', 'Annual_etr', 'Annual_precipitation',\n",
    "        'Annual_tempreture', 'pr', 'tmmn', 'vs', 'fm100', 'fm1000', 'bi', 'vpd', 'erc', 'Elevation_1km', 'Aspect_1km', 'erc_Percentile', 'Slope_1km',\n",
    "        'TPI_1km', 'EVC', 'Evacuation', 'SDI', 'FRG', 'No_FireStation_5.0km', 'Mang_Name', 'GAP_Sts', 'GACC_PL', 'GDP', 'GHM', 'NDVI-1day', 'NPL',\n",
    "        'Popo_1km', 'road_county_dis', 'road_interstate_dis', 'road_common_name_dis', 'road_other_dis', 'road_state_dis', 'road_US_dis', 'RPL_THEMES',\n",
    "        'RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4']\n",
    "FPA_FOD_c = FPA_FOD_W[cols]\n",
    "\n",
    "print('Before cleaning:')\n",
    "print('Rows: ', FPA_FOD_c.shape[0])\n",
    "print('columns: ', FPA_FOD_c.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Road\n",
    "road_cols = ['road_county_dis', 'road_interstate_dis', 'road_common_name_dis', 'road_other_dis', 'road_state_dis', 'road_US_dis']\n",
    "FPA_FOD_c['Distance2road'] = FPA_FOD_c[road_cols].min(axis = 1)\n",
    "FPA_FOD_r = FPA_FOD_c.drop(labels = road_cols, axis = 1)\n",
    "FPA_FOD_r = FPA_FOD_r.dropna(axis = 0, subset = ['Distance2road'])\n",
    "\n",
    "FPA_FOD_r.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Precentiles\n",
    "pers = ['erc_Percentile']\n",
    "for per in pers:\n",
    "     temp = FPA_FOD_r['erc_Percentile'].str.split(pat = '-', expand = True)\n",
    "     if temp.shape[1] == 2:\n",
    "          lower = temp[0].str.replace(pat = '>', repl = '').str.replace(pat = '%', repl = '').str.replace(pat = '<', repl = '').astype(float)\n",
    "          upper = temp[1].str.replace(pat = '>', repl = '').str.replace(pat = '%', repl = '').str.replace(pat = '<', repl = '').astype(float)\n",
    "          FPA_FOD_r[per] = np.mean(pd.concat(objs = [lower, upper], axis = 1), axis = 1)\n",
    "     else:\n",
    "          FPA_FOD_r[per] = temp[0].str.replace(pat = '>', repl = '').str.replace(pat = '%', repl = '').astype(float)\n",
    "\n",
    "# Management type\n",
    "j = 0\n",
    "labels = dict()\n",
    "for i in FPA_FOD_r['Mang_Name'].unique():\n",
    "     labels[i] = j\n",
    "     j+=1\n",
    "\n",
    "for i in FPA_FOD_r['Mang_Name'].unique():\n",
    "        FPA_FOD_r.loc[FPA_FOD_r['Mang_Name'] == i, 'Mang_Name'] = int(labels[i])\n",
    "print('\\nManagement type code:\\n', labels, '\\n')\n",
    "\n",
    "FPA_FOD_r.reset_index(drop = True, inplace = True)\n",
    "\n",
    "FPA_FOD_r = FPA_FOD_r.fillna(value = 0)\n",
    "\n",
    "# # GHM\n",
    "# FPA_FOD_r[FPA_FOD_r['GHM'] < 0] = 0\n",
    "\n",
    "print('After cleaning:')\n",
    "print('Rows: ', FPA_FOD_r.shape[0])\n",
    "print('columns: ', FPA_FOD_r.shape[1])\n",
    "\n",
    "# FPA_FOD_r.to_csv(path_or_buf = '/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/FPA_FOD_west.csv',\n",
    "#                  sep = ',', index = False)\n",
    "# FPA_FOD_r = FPA_FOD_r.drop(labels = 'STATE', axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to feed ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPA_FOD_r = pd.read_csv(filepath_or_buffer = '/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/FPA_FOD_west.csv',\n",
    "                        sep = ',', low_memory = False)\n",
    "FPA_FOD_r = FPA_FOD_r.drop(labels = 'STATE', axis = 1)\n",
    "unseen_data = FPA_FOD_r[FPA_FOD_r['NWCG_GENERAL_CAUSE'] == 'Missing data/not specified/undetermined']\n",
    "data = FPA_FOD_r[FPA_FOD_r['NWCG_GENERAL_CAUSE'] != 'Missing data/not specified/undetermined']\n",
    "\n",
    "j = 0\n",
    "labels = dict()\n",
    "for i in data['NWCG_GENERAL_CAUSE'].unique():\n",
    "     labels[i] = j\n",
    "     j+=1\n",
    "\n",
    "for i in data['NWCG_GENERAL_CAUSE'].unique():\n",
    "        data.loc[data['NWCG_GENERAL_CAUSE'] == i, 'NWCG_GENERAL_CAUSE'] = int(labels[i])\n",
    "print(labels)\n",
    "\n",
    "data = data.astype(np.float64)\n",
    "\n",
    "data_X = data.loc[:, data.columns != 'NWCG_GENERAL_CAUSE']\n",
    "data_y = data.loc[:, data.columns == 'NWCG_GENERAL_CAUSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = FPA_FOD_r.corr()\n",
    "corr.to_csv(path_or_buf = '/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/Corr.csv',\n",
    "            sep = ',',\n",
    "            index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = data_X.columns\n",
    "\n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(data_X.values, i)\n",
    "\t\t\t\t\t\tfor i in range(len(data_X.columns))]\n",
    "\n",
    "vif_data.to_csv(path_or_buf = '/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/vif_west.csv', sep = ',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size = 0.2, random_state = 42)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "\n",
    "y_train = pd.to_numeric(y_train['NWCG_GENERAL_CAUSE'])\n",
    "# y_valid = pd.to_numeric(y_valid['NWCG_GENERAL_CAUSE'])\n",
    "y_test = pd.to_numeric(y_test['NWCG_GENERAL_CAUSE'])\n",
    "\n",
    "print('train_X size: ', len(X_train))\n",
    "# print('valid_X size: ', len(X_valid))\n",
    "print('test_X size: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train, evaluation, and text distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1 = sns.histplot(data = y_train,\n",
    "                   stat = 'percent',\n",
    "                   discrete = True,\n",
    "                   kde = False,\n",
    "                   ax = ax1)\n",
    "ax1.grid()\n",
    "ax1.set_title('Train portion')\n",
    "ax1.bar_label(ax1.containers[0])\n",
    "\n",
    "# ax2 = sns.histplot(data = y_valid,\n",
    "#                    stat = 'percent',\n",
    "#                    discrete = True,\n",
    "#                    kde = False,\n",
    "#                    ax = ax2)\n",
    "# ax2.grid()\n",
    "# ax2.set_title('Valid portion')\n",
    "# ax2.bar_label(ax2.containers[0])\n",
    "\n",
    "ax3 = sns.histplot(data = y_test,\n",
    "                   stat = 'percent',\n",
    "                   discrete = True,\n",
    "                   kde = False,\n",
    "                   ax = ax3)\n",
    "ax3.grid()\n",
    "ax3.set_title('Test portion')\n",
    "ax3.bar_label(ax3.containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_wght = dict()\n",
    "# for cls in data.NWCG_GENERAL_CAUSE.unique():\n",
    "#      cls_wght[cls] = len(data)/(len(data.NWCG_GENERAL_CAUSE.unique()) * len(data[data['NWCG_GENERAL_CAUSE'] == cls]))\n",
    "# cls_wght = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "#                                              classes = data.NWCG_GENERAL_CAUSE.unique(),\n",
    "#                                              y = data.NWCG_GENERAL_CAUSE)\n",
    "cls_wght = class_weight.compute_sample_weight(class_weight = 'balanced',\n",
    "                                              y = y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Class Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper parameters for XGboost were optimized using GridsearchCV.\n",
    "\n",
    "You can either train the model using this hyper parameters, or load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_XGb_ww = xgb.XGBClassifier(tree_method = 'hist',\n",
    "#                             base_score = 0.5,\n",
    "#                             booster = 'gbtree',\n",
    "#                             objective = 'multi:softprob',\n",
    "#                             num_class = 12,\n",
    "#                             max_depth = 14,\n",
    "#                             alpha = 0,\n",
    "#                             learning_rate = 0.08,\n",
    "#                             n_estimators = 1550,\n",
    "#                             seed = 42,\n",
    "#                             )\n",
    "# clf_XGb_ww.fit(X_train, y_train, sample_weight = cls_wght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(value = clf_XGb_ww,\n",
    "#             filename = '/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/clf_xgb_WW.pkl')\n",
    "clf_XGb_ww = joblib.load(filename = '/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/clf_xgb_WW.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test: %.2f%%\" % (clf_XGb_ww.score(X = X_test, y = y_test) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorcode_confusion_matrix(df_cm, title):\n",
    "    cmap = plt.cm.Reds\n",
    "\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.imshow(df_cm, cmap = cmap)\n",
    "    plt.colorbar()\n",
    "\n",
    "    for i in range(len(df_cm.index)):\n",
    "        for j in range(len(df_cm.columns)):\n",
    "            plt.text(i, j, f\"{df_cm.iloc[i, j]:.2f}\", ha = \"center\", va = \"center\", fontsize = 9) # Print decimal values\n",
    "            # plt.text(i, j, f\"{df_cm.iloc[i, j]}\", ha = \"center\", va = \"center\", fontsize = 9) # Print integer values\n",
    "\n",
    "    plt.xticks(np.arange(len(df_cm.columns)), df_cm.columns, rotation = 90)\n",
    "    plt.yticks(np.arange(len(df_cm.columns)), df_cm.index)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_test = clf_XGb_ww.predict(X = X_test)\n",
    "cm_test = confusion_matrix(y_true = y_test,\n",
    "                           y_pred = pred_y_test,\n",
    "                           labels = list(labels.values()),\n",
    "                           normalize = 'true')\n",
    "cm_test = pd.DataFrame(data = cm_test, index = list(labels.keys()), columns = list(labels.keys()))\n",
    "\n",
    "colorcode_confusion_matrix(df_cm = cm_test, title = 'Confusion matrix on test set')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Class Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper parameters for XGboost were optimized using GridsearchCV.\n",
    "\n",
    "You can either train the model using this hyper parameters, or load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_XGb = xgb.XGBClassifier(tree_method = 'hist',\n",
    "#                             base_score = 0.5,\n",
    "#                             booster = 'gbtree',\n",
    "#                             objective = 'multi:softprob',\n",
    "#                             num_class = 12,\n",
    "#                             max_depth = 11,\n",
    "#                             alpha = 0,\n",
    "#                             learning_rate = 0.13,\n",
    "#                             n_estimators = 1200,\n",
    "#                             seed = 42,\n",
    "#                             )\n",
    "# clf_XGb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(value = clf_XGb,\n",
    "#             filename = '/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/clf_xgb.pkl')\n",
    "clf_XGb = joblib.load(filename = '/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/clf_xgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test: %.2f%%\" % (clf_XGb.score(X = X_test, y = y_test) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)\n",
    "\n",
    "for class_of_interest in range(12):\n",
    "    class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "    metrics.RocCurveDisplay.from_predictions(\n",
    "        y_onehot_test[:, class_id],\n",
    "        clf_XGb.predict_proba(X_test)[:, class_id],\n",
    "        name = f\"{class_of_interest} vs the rest\",\n",
    "        color = \"darkorange\",\n",
    "        plot_chance_level = True,\n",
    "        )\n",
    "    plt.axis(\"square\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"One-vs-Rest ROC curves:\\nVirginica vs (Setosa & Versicolor)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_prob = clf_XGb.predict_proba(X = X_test, # (numpy array, pandas DataFrame, H2O DataTable's Frame , scipy.sparse, list of lists of int or float of shape = [n_samples, n_features]) – Input features matrix.\n",
    "                                 )\n",
    "cls_prob = pd.DataFrame(data = cls_prob, columns = labels)\n",
    "cls_prob.to_csv(path_or_buf = '/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/cls_prob_xgb.csv',\n",
    "                sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_test = clf_XGb.predict(X = X_test)\n",
    "cm_test = confusion_matrix(y_true = y_test,\n",
    "                           y_pred = pred_y_test,\n",
    "                           labels = list(labels.values()),\n",
    "                           normalize = 'true')\n",
    "cm_test = pd.DataFrame(data = cm_test, index = list(labels.keys()), columns = list(labels.keys()))\n",
    "\n",
    "colorcode_confusion_matrix(df_cm = cm_test, title = 'Confusion matrix on test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Shapley Values for Feature Importance\n",
    "# =============================================================================\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(clf_XGb, approximate = True)\n",
    "shap_values = explainer.shap_values(X_test)#.sample(frac = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swap_dict(d):\n",
    "    return {v: k for k, v in d.items()}\n",
    "swap_labels = get_swap_dict(labels)\n",
    "newCmap = ['#a6cee3','#1f78b4', '#b2df8a', '#33a02c', '#fb9a99', '#e31a1c',\n",
    "           '#fdbf6f', '#ff7f00', '#cab2d6', '#6a3d9a', '#ffff99', '#b15928']\n",
    "\n",
    "newCmap = LinearSegmentedColormap.from_list(name = '',\n",
    "                                            colors = newCmap,\n",
    "                                            N = 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = {\n",
    "             'Popo_1km': 'Pop_1km',\n",
    "             '11': 'Open Water',\n",
    "             '12': 'Snow/Ice',\n",
    "             '13': 'Developed-Upland Deciduous Forest',\n",
    "             '14': 'Developed-Upland Evergreen Forest',\n",
    "             '15': 'Developed-Upland Mixed Forest',\n",
    "             '16': 'Developed-Upland Herbaceous',\n",
    "             '17': 'Developed-Upland Shrubland',\n",
    "             '22': 'Developed - Low Intensity',\n",
    "             '23': 'Developed - Medium Intensity',\n",
    "             '24': 'Developed - High Intensity',\n",
    "             '25': 'Developed-Roads',\n",
    "             '31': 'Barren',\n",
    "             '32': 'Quarries-Strip Mines-Gravel Pits-Well and Wind Pads',\n",
    "             '61': 'NASS-Vineyard',\n",
    "             '63': 'NASS-Row Crop-Close Grown Crop',\n",
    "             '64': 'NASS-Row Crop',\n",
    "             '65': 'NASS-Close Grown Crop',\n",
    "             '68': 'NASS-Wheat',\n",
    "             '69': 'NASS-Aquaculture',\n",
    "             '100': 'Sparse Vegetation Canopy',\n",
    "             '200': 'Shrub Cover',\n",
    "             '300': 'Herb Cover',\n",
    "            }\n",
    "feat_name = X_train.columns.to_list()\n",
    "for idx, item in enumerate(X_train.columns):\n",
    "    if item in feat_dict:\n",
    "        feat_name[idx] = feat_dict[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "plt_shap = shap.summary_plot(shap_values,                       # Use Shap values array\n",
    "                             features = X_train,                # Use training set features\n",
    "                             feature_names = feat_name,         # Use column names\n",
    "                             show = False,                      # Set to false to output to folder\n",
    "                             class_names = swap_labels,         # Converting the class from int to orginal labels [Dict class]\n",
    "                             color = newCmap,                   # New set of color, it should be in 'LinearSegmentedColormap' class\n",
    "                             color_bar_label = 'Feature value',\n",
    "                             plot_size = (15,5),                # Change plot size\n",
    "                            #  class_inds = 'original',         # It will always keep the class labels in the same order\n",
    "                             )\n",
    "plt.savefig(f'/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/Plots/summary_plot_West_xgb.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "sh_val = np.zeros(shape = shap_values[0].shape)\n",
    "\n",
    "for cls in range(len(shap_values)):\n",
    "    sh_val = sh_val + shap_values[cls]\n",
    "\n",
    "rf_resultX = pd.DataFrame(sh_val, columns = feature_names)\n",
    "\n",
    "vals = np.abs(rf_resultX.values).mean(0)\n",
    "\n",
    "shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                columns = ['col_name', 'feature_importance_vals'])\n",
    "shap_importance.sort_values(by = ['feature_importance_vals'],\n",
    "                            ascending = False,\n",
    "                            inplace = True)\n",
    "shap_importance.to_csv(path_or_buf = '/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/feat_import_xgb.csv',\n",
    "                       sep = ',',\n",
    "                       index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    shap.summary_plot(shap_values = shap_values[i],     # Use Shap values array\n",
    "                      plot_type = 'violin',             # “dot” (default for single output), “bar” (default for multi-output), “violin”\n",
    "                      features = X_test,               # Use training set features\n",
    "                      feature_names = feat_name,        # Use column names\n",
    "                      show = False,                     # Set to false to output to folder\n",
    "                      plot_size = (20, 10),                 # Change plot size: None, 'auto', (10, 7)\n",
    "                      cmap = \"plasma\",\n",
    "                      # title = f'feature importance for {swap_labels[0]} cause',\n",
    "                      )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(-1, 1) \n",
    "    plt.title(label = f'Feature importance for {swap_labels[i]} cause',\n",
    "              loc = 'center',\n",
    "            #   pad = 15,\n",
    "              )\n",
    "    nam = swap_labels[i].replace('/', ' or ')\n",
    "    plt.savefig(f'/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/Plots/West_xgb_{nam}.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = glob.glob(pathname = f'/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/Plots/West_xgb_*.png')\n",
    "\n",
    "ii = [0, 2, 4, 6, 8, 10]\n",
    "count = 0\n",
    "Horizental_frame = [[],[],[],[],[],[]]\n",
    "for i in ii:\n",
    "    image1 = cv.imread(image_list[i])\n",
    "    image2 = cv.imread(image_list[i + 1])\n",
    "    Horizental_frame[count] = cv.hconcat((image1, image2)) # or vconcat for vertical concatenation\n",
    "    count += 1\n",
    "final_frame = cv.vconcat((Horizental_frame[0],\n",
    "                          Horizental_frame[1],\n",
    "                          Horizental_frame[2],\n",
    "                          Horizental_frame[3],\n",
    "                          Horizental_frame[4],\n",
    "                          Horizental_frame[5]))\n",
    "\n",
    "cv.imwrite(f'/bsuhome/yavarpourmohamad/scratch/Unknown_classification/Engineered Features/Plots/per_cause_West_xgb.png', final_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in image_list:\n",
    "    os.remove(i)\n",
    "print(f'All {len(image_list)} files deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_feat = shap_importance.reset_index(drop = True).loc[0:19, 'col_name'].values\n",
    "plt_dt = pd.concat([y_test, X_test], axis = 1)[np.append('NWCG_GENERAL_CAUSE', shap_feat)].sample(n = 50, random_state = 42)\n",
    "plt_dt = plt_dt.set_index(keys = 'NWCG_GENERAL_CAUSE')\n",
    "# Calculate the distance between each sample\n",
    "Z = Sciplot.linkage(y = plt_dt,\n",
    "                    method = 'ward')\n",
    "# other methods: single, complete, average, weighted, centroid, median\n",
    "\n",
    "# Plot with Custom leaves\n",
    "Sciplot.dendrogram(Z = Z,\n",
    "                   leaf_rotation = 90,\n",
    "                   leaf_font_size = 8,\n",
    "                   labels = plt_dt.index)\n",
    "\n",
    "# Show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_feat = shap_importance.reset_index(drop = True).loc[0:19, 'col_name'].values\n",
    "sctr_plt = pd.concat([y_test, X_test], axis = 1)[np.append('NWCG_GENERAL_CAUSE', shap_feat)].sample(n = 500, random_state = 42)\n",
    "sctr_plt[sctr_plt['GHM'] < 0] = 0\n",
    "\n",
    "for i in sctr_plt['NWCG_GENERAL_CAUSE'].unique():\n",
    "        sctr_plt.loc[sctr_plt['NWCG_GENERAL_CAUSE'] == i, 'NWCG_GENERAL_CAUSE'] = swap_labels[i]\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(10, 5)})\n",
    "ax  = sns.scatterplot(data = sctr_plt,\n",
    "                  x = 'GHM',\n",
    "                  y = 'Elevation_1km',\n",
    "                  hue = 'NWCG_GENERAL_CAUSE',\n",
    "                  alpha = 0.6)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor = (1, 1))\n",
    "plt.xlabel(xlabel = 'Global Human Modification')\n",
    "plt.ylabel(ylabel = 'Elevation (m)')\n",
    "plt.show()\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(10, 5)})\n",
    "ax  = sns.kdeplot(data = sctr_plt,\n",
    "                  x = 'GHM',\n",
    "                  y = 'Elevation_1km',\n",
    "                  hue = 'NWCG_GENERAL_CAUSE',\n",
    "                  shade = True,\n",
    "                  alpha = 0.75)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor = (1, 1))\n",
    "plt.xlabel(xlabel = 'Global Human Modification')\n",
    "plt.ylabel(ylabel = 'Elevation (m)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(10, 5)})\n",
    "ax  = sns.scatterplot(data = sctr_plt,\n",
    "                      x = 'DISCOVERY_DOY',\n",
    "                      y = 'Elevation_1km',\n",
    "                      hue = 'NWCG_GENERAL_CAUSE',\n",
    "                    #   palette = newCmap,\n",
    "                      alpha = 0.6)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor = (1, 1))\n",
    "plt.xlabel(xlabel = 'Day of Year')\n",
    "plt.ylabel(ylabel = 'Elevation (m)')\n",
    "plt.show()\n",
    "\n",
    "ax  = sns.kdeplot(data = sctr_plt,\n",
    "                  x = 'DISCOVERY_DOY',\n",
    "                  y = 'Elevation_1km',\n",
    "                  hue = 'NWCG_GENERAL_CAUSE',\n",
    "                  shade = True,\n",
    "                  alpha = 0.75)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor = (1, 1))\n",
    "plt.xlabel(xlabel = 'Day of Year')\n",
    "plt.ylabel(ylabel = 'Elevation (m)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax  = sns.scatterplot(data = sctr_plt,\n",
    "                      x = 'DISCOVERY_DOY',\n",
    "                      y = 'GHM',\n",
    "                      hue = 'NWCG_GENERAL_CAUSE',\n",
    "                    #   palette = newCmap,\n",
    "                      alpha = 0.6)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor = (1, 1))\n",
    "plt.xlabel(xlabel = 'Day of Year')\n",
    "plt.ylabel(ylabel = 'Global Human Modification')\n",
    "plt.show()\n",
    "\n",
    "ax  = sns.kdeplot(data = sctr_plt,\n",
    "                  x = 'DISCOVERY_DOY',\n",
    "                  y = 'GHM',\n",
    "                  hue = 'NWCG_GENERAL_CAUSE',\n",
    "                  shade = True,\n",
    "                  alpha = 0.75)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor = (1, 1))\n",
    "plt.xlabel(xlabel = 'Day of Year')\n",
    "plt.ylabel(ylabel = 'Global Human Modification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = sctr_plt[['NWCG_GENERAL_CAUSE', 'DISCOVERY_DOY', 'Elevation_1km', 'GHM']], # .sample(n = 1000, random_state = 42),\n",
    "             kind = 'scatter',\n",
    "             hue = 'NWCG_GENERAL_CAUSE',\n",
    "             plot_kws = dict(s = 10, edgecolor = 'white', linewidth = 0.8)\n",
    "             )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default plot\n",
    "sns.clustermap(data = sctr_plt[['NWCG_GENERAL_CAUSE', 'DISCOVERY_DOY', 'Elevation_1km', 'GHM']].sample(n = 500, random_state = 42).set_index(keys = 'NWCG_GENERAL_CAUSE'),\n",
    "            #    z_score = 1, # Either 0 (rows) or 1 (columns)\n",
    "               standard_scale = 1, # Either 0 (rows) or 1 (columns)\n",
    "               metric = 'euclidean',\n",
    "               method = 'ward'\n",
    "               )\n",
    "\n",
    "# Show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in ['GHM', 'Elevation_1km', 'DISCOVERY_DOY']:\n",
    "    sns.violinplot(data = sctr_plt,\n",
    "                x = 'NWCG_GENERAL_CAUSE',\n",
    "                y = feat,\n",
    "                order = list(labels.keys()),\n",
    "                palette = sns.color_palette())\n",
    "    plt.xticks(rotation = 70)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating a responsive plot\n",
    "# %matplotlib widget\n",
    "\n",
    "# Import libraries\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "# Creating figure\n",
    "fig = plt.figure(figsize = (16, 9))\n",
    "# ax = plt.axes(projection = '3d')\n",
    "ax = mplot3d.Axes3D(fig)\n",
    "\n",
    "# Add x, y gridlines \n",
    "ax.grid(b = True,\n",
    "        color ='black', \n",
    "\t\tlinestyle ='-.',\n",
    "        linewidth = 0.3, \n",
    "\t\talpha = 0.2) \n",
    "\n",
    "sctr_plt_int = sctr_plt.copy()\n",
    "for i in sctr_plt_int['NWCG_GENERAL_CAUSE'].unique():\n",
    "        sctr_plt_int.loc[sctr_plt_int['NWCG_GENERAL_CAUSE'] == i, 'NWCG_GENERAL_CAUSE'] = labels[i]\n",
    "\n",
    "sctr_plt_int = sctr_plt_int.sample(n = 500, random_state = 42)\n",
    "\n",
    "# Creating plot\n",
    "sctt = ax.scatter3D(xs = sctr_plt_int.GHM,\n",
    "\t\t\t\t\tys = sctr_plt_int.Elevation_1km,\n",
    "\t\t\t\t\tzs = sctr_plt_int.DISCOVERY_DOY,\n",
    "\t\t\t\t\talpha = 0.8,\n",
    "\t\t\t\t\tc = sctr_plt_int.NWCG_GENERAL_CAUSE, \n",
    "\t\t\t\t\tcmap = newCmap,\n",
    "\t\t\t\t\tmarker = 'o')\n",
    "\n",
    "plt.title('GHM vs Elevation vs Discovery doy')\n",
    "\n",
    "ax.set_xlabel('Global Human Modification', fontweight ='bold') \n",
    "ax.set_ylabel('Elevation', fontweight ='bold') \n",
    "ax.set_zlabel('Discovery day of year', fontweight ='bold')\n",
    "fig.colorbar(sctt, ax = ax, shrink = 0.5, aspect = 5)\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
